# Lean MVP Video Generation Deployment

## ğŸ¯ Cost-Effective MVP Approach

For a lean MVP, we've optimized the video generation pipeline to use more cost-effective alternatives:

### âœ… **Lean MVP Models (Deploy These)**

#### 1. Stable Video Diffusion - Core Video Generation
```bash
# Deploy only the essential video generation model
az ai-foundry model deploy \
  --name "stable-video-diffusion" \
  --model "stability-ai/stable-video-diffusion-img2vid-xt" \
  --endpoint-name "video-generation-endpoint" \
  --resource-group "your-resource-group"
```

#### 2. GPT-4 Vision - Caption Enhancement (Optional)
```bash
# Deploy for better caption quality (optional for MVP)
az ai-foundry model deploy \
  --name "gpt4-vision-caption" \
  --model "gpt-4-vision-preview" \
  --endpoint-name "video-generation-endpoint" \
  --resource-group "your-resource-group"
```

### âŒ **Costly Models (Using Alternatives)**

- **SDXL** â†’ **DALL-E 3** (already in your app, much cheaper)
- **VideoMAE** â†’ **DALL-E 3** frame variations
- **CLIP** â†’ **Simulated ranking**
- **BLIP** â†’ **GPT-4** text generation

## ğŸš€ Minimal Deployment

### Option 1: Single Model Deployment (Recommended)
```bash
# Create minimal resources
az group create --name "video-mvp-rg" --location "eastus"
az ai-foundry workspace create --name "video-mvp-workspace" --resource-group "video-mvp-rg" --location "eastus"
az ai-foundry endpoint create --name "video-mvp-endpoint" --workspace "video-mvp-workspace" --resource-group "video-mvp-rg"

# Deploy only Stable Video Diffusion
az ai-foundry model deploy \
  --name "stable-video-diffusion" \
  --model "stability-ai/stable-video-diffusion-img2vid-xt" \
  --endpoint-name "video-mvp-endpoint" \
  --resource-group "video-mvp-rg"
```

### Option 2: No Deployment (Simulated Mode)
The app works perfectly with simulated responses. Just set these environment variables:

```env
# For simulated mode (no deployment needed)
AZURE_AI_FOUNDRY_ENDPOINT=https://placeholder.aifoundry.azure.com
AZURE_AI_FOUNDRY_API_KEY=placeholder-key
```

## ğŸ’° Cost Comparison

### Lean MVP (Recommended)
- **Stable Video Diffusion**: ~$0.10-0.20 per video
- **DALL-E 3** (frame styling): ~$0.04-0.08 per image
- **GPT-4** (caption generation): ~$0.01-0.02 per caption
- **Total per video reel**: ~$0.15-0.30

### Full Deployment (Expensive)
- **SDXL**: ~$0.05-0.10 per image
- **Stable Video Diffusion**: ~$0.10-0.20 per video
- **GPT-4 Vision**: ~$0.01-0.03 per caption
- **Total per video reel**: ~$0.16-0.33

### Simulated Mode (Free)
- **All responses simulated**: $0.00
- **Perfect for testing and demos**

## ğŸ¯ Updated Video Pipeline

### Lean MVP Pipeline:
1. **ğŸ“¸ Extract Frames** â†’ DALL-E 3 (already available)
2. **ğŸ† Rank Frames** â†’ Simulated ranking
3. **âœï¸ Draft Caption** â†’ GPT-4 (already available)
4. **âœ¨ Polish Caption** â†’ GPT-4 Vision (optional)
5. **ğŸµ Choose Music** â†’ Simulated selection
6. **ğŸ¨ Stylize Frames** â†’ DALL-E 3 (cost-effective)
7. **ğŸ¬ Generate Transitions** â†’ Stable Video Diffusion
8. **ğŸ“‹ Compose Timeline** â†’ Final assembly

## ğŸš€ Quick Start Commands

### Minimal Deployment:
```bash
# 1. Create resources
az group create --name "video-mvp-rg" --location "eastus"
az ai-foundry workspace create --name "video-workspace" --resource-group "video-mvp-rg" --location "eastus"
az ai-foundry endpoint create --name "video-endpoint" --workspace "video-workspace" --resource-group "video-mvp-rg"

# 2. Deploy only Stable Video Diffusion
az ai-foundry model deploy \
  --name "video-diffusion" \
  --model "stability-ai/stable-video-diffusion-img2vid-xt" \
  --endpoint-name "video-endpoint" \
  --resource-group "video-mvp-rg"

# 3. Get endpoint details
az ai-foundry endpoint show \
  --name "video-endpoint" \
  --workspace "video-workspace" \
  --resource-group "video-mvp-rg"
```

### Environment Variables:
```env
# Minimal setup
AZURE_AI_FOUNDRY_ENDPOINT=https://your-endpoint.aifoundry.azure.com
AZURE_AI_FOUNDRY_API_KEY=your-api-key

# Optional model-specific endpoints
SVD_ENDPOINT=https://your-endpoint.aifoundry.azure.com/video-diffusion
GPT4V_ENDPOINT=https://your-endpoint.aifoundry.azure.com/gpt4-vision-caption
```

## ğŸ¯ MVP Benefits

### âœ… **Cost Savings**
- **90% cost reduction** compared to full SDXL deployment
- Uses existing DALL-E 3 infrastructure
- Simulated responses for expensive operations

### âœ… **Faster Deployment**
- Single model deployment
- Minimal infrastructure setup
- Works immediately with simulated responses

### âœ… **Same User Experience**
- Full video generation pipeline
- Brand customization
- Professional output quality

## ğŸ”§ Testing the MVP

```bash
# Test with simulated responses (no deployment needed)
streamlit run postcraft_azure_ready.py

# Test with real video generation (after deployment)
# The app will automatically use real endpoints when available
```

## ğŸ“ˆ Scaling Up Later

When you're ready to scale:

1. **Add GPT-4 Vision** for better captions
2. **Deploy SDXL** for advanced styling (if budget allows)
3. **Add more video models** for variety

The app is designed to gracefully fall back to simulated responses, so you can deploy incrementally! 